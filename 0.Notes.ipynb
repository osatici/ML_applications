{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# General Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation\n",
    "\n",
    "Confusion matrix\n",
    "\n",
    "Sensitivity:true positive rate or recall, (TP/(TP+FN)): proportion of actual positives that are identified correctly\n",
    "Specificity: true negative rate. (TN/(TN+FP)). out of the actual negatives that are identified correctly\n",
    "Precision: (TP/(TP+FP)) :proportion of positive identifications that were actually correct.\n",
    "F1-score: 2x((precisionxrecall)/(precision+recall)):  balances precision and recall\n",
    "\n",
    "Bias (underfitting) vs variance (overfitting)\n",
    "\n",
    "ROC (Receiver Operating Characteristic) curve and AUC (Area Under the Curve): good when cost of FP and FN are different, mostly for logistic regression\n",
    "ROC curve: y axix sensitivity or recall (TPR= TP/(TP+FN)), x axis  (FPR = FP/(TN+FP)), you draw this as you increase the threshold and construct the confusion matrix\n",
    "\tthe proportion of correct classification of actual positives vs the poroportion of incorrect classification of actual negatives\n",
    "\twe want TPR to be high, FPR to be small\n",
    "AUC is the area under the ROC curve. larger is better. you can calculate AUC for different methods and select larger valued one.\n",
    "\n",
    "Entropy:quantifies similarities and differences\n",
    "\tSurprise: inverse related to probability: log(1/probability)\n",
    "\tTotal surprise: heads qty x head surprise + tails qty x tails surprise\n",
    "\tExpected value of surprise: Entropy: total surprise/ (heads qty+tails qty)\n",
    "\tIn short, entropy:heads prob x head surprise + tails prob x tails surprise\n",
    "\n",
    "\tShows purity of a node. Nodes with higher entropy are more impure, and thus the decision tree algorithm will attempt to split these nodes to increase the overall purity (decrease entropy) of the children nodes. \n",
    "\n",
    "R^2: metrical correlation\n",
    "\tRegular correlation:R: \n",
    "\tCov(X,Y)=  \\sum {(xi-x_bar)*(yi-y_bar)} / N-1\n",
    "\tR = Cor(X,Y) = Cov(X,Y) / sqrt(Var(X)*VAR(Y))\n",
    "\tR = 0.7 is twice as good as 0.5\n",
    "\n",
    "\tR^2 = (VAR(X,Y)mean - VAR(X,Y)fitted line) / VAR(X,Y)mean\n",
    "\tVAR here is the sum of squared distances from the mean/fitted line \n",
    "\tR = 0.7 is 1.4 better than 0.5\n",
    "\tshows how a fitted linear regression line is better than a horizontal line (mean) in percentages\n",
    "\tAlso shows that the relationship between two variable explains R^2 percent of the variation in data.\n",
    "\tyou can just get the squar of R for it too?.\n",
    "\n",
    "Mutual information:\n",
    "\tR^2 only works with continuous data\n",
    "\tLike R^2 mutual information is a numeric value giving us a sense of how closely related two variables are\n",
    "\tweird formulation, look it up\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
